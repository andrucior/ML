Adam optimizer used
epoch limit: 100
patience: 12
10% of available unknown data used for training
70-20-10 train-test-val split
initial weights used for every class
training terminated after reaching the epoch limit
end loss: 0.65
end validation: 82.5%
Two dropout layers used during training

Model described in raport, point 6.1.4